{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UE-xgaF3As6q"
      },
      "source": [
        "This notebook is to introduce you to curve fitting in python. The majority of this code is to introduce you to the various components of the fitting code. We use the plots/fits of Lecture 5 to do this. For lab, you can copy the code in the cell labeled #SUMMARY FOR LAB, and edit the various components. You won't need the other cells for lab."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0Cx9MXBY9rXy"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ILsPUu1U9yaZ"
      },
      "outputs": [],
      "source": [
        "#This cell makes the data for the example plots shown in lecture 5.\n",
        "#The \"data\" in that lecture was generated by this code--it was not real.\n",
        "\n",
        "npts = 15; #Number of data points to generate\n",
        "xmin = 0; #min value of data range\n",
        "xmax = 1.7; # max value of data range\n",
        "x = np.linspace(xmin, xmax, npts); #this is the independent variable\n",
        "\n",
        "#Let's make a \"data curve\" that looks like y = x + x^2 + x^3, and then we'll add a little noise.\n",
        "y   = x+ x**2 + x**3 ; #this is a smooth curve, no noise.\n",
        "y = y + np.random.normal(0, 0.4, npts) # now we add a little noise. The details of how we add noise aren't super important. This is to mimick experimental uncertainty.\n",
        "\n",
        "dy =0.1+np.abs(np.random.normal(0, 0.4, npts)); #this variable will represent the uncertainty in each data point. Again here the details aren't important."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b-1PXYoz-89R"
      },
      "outputs": [],
      "source": [
        "#This cell makes a plot of the data. This code should look somewhat familiar from our python plotting assignment.\n",
        "plt.rcParams.update({'font.size': 15})\n",
        "plt.errorbar(x, y, yerr = dy, xerr = None, fmt = 'k.');\n",
        "plt.xlabel('Indep Variable');\n",
        "plt.ylabel('Dep Variable');"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GYte3B05_CWQ"
      },
      "outputs": [],
      "source": [
        "#Let's try fitting this data with a line. We'll use the function in numpy called \"polyfit\" to do a polynomial fit (of degree 2).\n",
        "#Polyfit can do any order polynomial. Here we choose a polynomial of order 1, or a line.\n",
        "#Polyfit makes a chi-squared, and minimizes it. Then it spits out the coefficients of that polynomial in the resulting numpy array.\n",
        "\n",
        "polynomial_degree = 1\n",
        "p = np.polyfit(x, y, polynomial_degree, w = 1/dy ) ;  # this line performs the fit. p contains the coefficients of the polynomial. The syntax is not super obvious, I would say. The piece \"w = \" specifies the \"weight\" to apply to each of the residuals. As in lecture, the weight should be set to 1/(uncertainty).\n",
        "f = np.poly1d(p) ; #This is a super handy line. It converts the list of polynomial coefficients (p), into the corresponding polynomial function (f).\n",
        "\n",
        "#Here's how we can use f:\n",
        "\n",
        "plt.errorbar(x, y, yerr = dy, xerr = None, fmt = 'k.'); #First make the plot from before.\n",
        "\n",
        "plt.plot(x, f(x), 'r-'); # Now add to that plot the fit line. Notice we can just use f like a fucntion! It's our fit function.\n",
        "\n",
        "plt.xlabel('Indep Variable');\n",
        "plt.ylabel('Dep Variable');\n",
        "plt.legend(['Fit', 'Data']);"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5ioz4IgPCgCT"
      },
      "outputs": [],
      "source": [
        "#Aside: What if we didn't include the uncertainties?\n",
        "polynomial_degree = 1\n",
        "p_no_unc = np.polyfit(x, y, polynomial_degree) ;  # same as before, except we've left out 'w', which tells the fitter what the uncertainties are. When we leave it out, it treats all points as if they have the same uncertainty.\n",
        "f_no_unc = np.poly1d(p_no_unc) ; #same as before\n",
        "plt.errorbar(x, y, yerr = dy, xerr = None, fmt = 'k.'); #same as before\n",
        "plt.plot(x, f_no_unc(x), 'r-'); #same as before\n",
        "plt.xlabel('Indep Variable');\n",
        "plt.ylabel('Dep Variable');\n",
        "plt.legend(['Fit', 'Data']);\n",
        "#Look at the plot closely: Note that this changes the fit! the fit doesn't try as hard to hit the points with very small uncertainty."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "clzWE0d8GuVZ"
      },
      "outputs": [],
      "source": [
        "#What if we try to fit with a parabola? From here on out we will always include uncertainties in our fits (meaning, we will make a proper chi-squared.)\n",
        "polynomial_degree = 2\n",
        "p_parabola = np.polyfit(x, y, polynomial_degree, w = 1/dy) ;  # now we include uncertainties again.\n",
        "f_parabola = np.poly1d(p_parabola)\n",
        "plt.errorbar(x, y, yerr = dy, xerr = None, fmt = 'k.'); #same as before\n",
        "plt.plot(x, f_parabola(x), 'r-') #now plot the parabola and the line.\n",
        "plt.plot(x, f(x), 'g-') #same as before\n",
        "plt.xlabel('Indep Variable')\n",
        "plt.ylabel('Dep Variable')\n",
        "plt.legend(['Parabola', 'Linear', 'Data']);\n",
        "#Look at the plot closely: Note that this changes the fit! the fit doesn't try as hard to hit the points with very small uncertainty."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JsX4r4s-HlTP"
      },
      "outputs": [],
      "source": [
        "#Let's try cubic now\n",
        "polynomial_degree = 3\n",
        "p_cubic = np.polyfit(x, y, polynomial_degree, w = 1/dy) ;  # now we include uncertainties again.\n",
        "f_cubic = np.poly1d(p_cubic)\n",
        "plt.errorbar(x, y, yerr = dy, xerr = None, fmt = 'k.'); #same as before\n",
        "plt.plot(x, f_cubic(x), 'b-')\n",
        "plt.plot(x, f_parabola(x), 'r-') #now plot the parabola and the line.\n",
        "plt.plot(x, f(x), 'g-') #same as before\n",
        "plt.xlabel('Indep Variable')\n",
        "plt.ylabel('Dep Variable')\n",
        "plt.legend(['Cubic', 'Parabola', 'Linear', 'Data']);\n",
        "#Look at the plot closely: Note that this changes the fit! the fit doesn't try as hard to hit the points with very small uncertainty."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "INJhtlXsJNFB"
      },
      "outputs": [],
      "source": [
        "#Since polyfit makes it's own chi-squared internally and hides it from us, if we want to see the actual chi-squared value resulting from the fit, we need to compute it ourselves.\n",
        "#To make this easy for you in the future, I'll just define a new function to do this. If you're confused about this cell, don't worry, you'll see how to compute chi-squared below.\n",
        "#Just make sure you evaluate this cell (or the equivalent one below in the summary) before trying to compute chi-squared yourself.\n",
        "\n",
        "def chi2(data, unc, model): #define (\"def\") a function called chi2 that takes as input variables named data, unc, and model, in that order.\n",
        "  #here \"data\" is  the data values, unc is the uncertainty in the data, and model is the function we fit with polyfit.\n",
        "  #These three varaibles should be numpy arrays--that is, numpy lists of data. See an example below to see what I mean.\n",
        "  N = 1.00*len(data) #Look at the length of the x array to see how many data points you have. Multiply it by \"1.00\" to convert it into a \"float\" data type. If you don't know what this is, it's not really important.\n",
        "  result = np.sum((data-model)**2/unc**2)/N # make chi-squared: sum up the (data-model)^2/unc^2, then divide it by N\n",
        "  return result #send back to the user the value of chi2\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IV3nP6ibNvFI"
      },
      "outputs": [],
      "source": [
        "#Once we've defined that function, we can use it to compute a few values of chi2 for our different values quite easily.\n",
        "\n",
        "linear_chi2 = chi2(y, dy, f(x))\n",
        "parab_chi2 = chi2(y, dy, f_parabola(x))\n",
        "cubic_chi2 = chi2(y, dy, f_cubic(x))\n",
        "print('The chi^2 value of the linear model is: ', linear_chi2)\n",
        "print('The chi^2 value of the parabolic model is: ', parab_chi2)\n",
        "print('The chi^2 value of the cubic model is: ', cubic_chi2)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lA_G8DUzNvxj"
      },
      "outputs": [],
      "source": [
        "#We can also make plots of the residuals.\n",
        "res_linear = y-f(x); #make the residuals for the linear fit.\n",
        "plt.errorbar(x, res_linear, yerr = dy, xerr = None, fmt = 'ko'); #plot it, using the same error bars as before\n",
        "plt.plot(x, 0*x, '--') #this code makes a dotted horizontal line at zero which is just there to guide the eye.\n",
        "plt.xlabel('Indep Variable');\n",
        "plt.ylabel('Residuals [data-model]');"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BN2q4a7IRpaA"
      },
      "outputs": [],
      "source": [
        "#Make the residuals plot for the quadratic model.\n",
        "res_parab = y-f_parabola(x);\n",
        "plt.errorbar(x, res_parab, yerr = dy, xerr = None, fmt = 'ko');\n",
        "plt.plot(x, 0*x, '--')\n",
        "plt.xlabel('Indep Variable')\n",
        "plt.ylabel('Residuals [data-model]')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DLXFc2DvSlVr"
      },
      "outputs": [],
      "source": [
        "#Do the same for the cubic data.\n",
        "res_cubic = y-f_cubic(x);\n",
        "plt.errorbar(x, res_cubic, yerr = dy, xerr = None, fmt = 'ko');\n",
        "plt.plot(x, 0*x, '--')\n",
        "plt.xlabel('Indep Variable');\n",
        "plt.ylabel('Residuals [data-model]');"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3IRbsyCVSwGb"
      },
      "outputs": [],
      "source": [
        "polynomial_degree = 30\n",
        "x_fine = np.linspace(np.min(x), np.max(x), 300)\n",
        "p_overfit = np.polyfit(x, y, polynomial_degree, w = 1/dy) ;  # now we include uncertainties again.\n",
        "f_overfit = np.poly1d(p_overfit)\n",
        "plt.errorbar(x, y, yerr = dy, xerr = None, fmt = 'k.'); #same as before\n",
        "plt.plot(x_fine, f_overfit(x_fine), 'b-')\n",
        "plt.ylim([-2, 10])\n",
        "plt.xlabel('Indep Variable');\n",
        "plt.ylabel('Dep Variable');\n",
        "plt.legend(['Overfit!', 'Data']);"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A4EGp51MTVNV"
      },
      "outputs": [],
      "source": [
        "#Overfit residuals\n",
        "res_overfit = y-f_overfit(x);\n",
        "plt.errorbar(x, res_overfit, yerr = dy, xerr = None, fmt = 'ko');\n",
        "plt.plot(x, 0*x, '--');\n",
        "plt.xlabel('Indep Variable');\n",
        "plt.ylabel('Residuals [data-model]');\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n1pM58FcUBBz"
      },
      "outputs": [],
      "source": [
        "overfit_chi2 = chi2(y, dy, f_overfit(x))\n",
        "print('The chi^2 value of the overfit model is: ', overfit_chi2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L08VoRGTUOVz"
      },
      "outputs": [],
      "source": [
        "#SUMMARY FOR LAB\n",
        "# if you want more explanation of what's going on here, see the cells above.\n",
        "\n",
        "#Import stuff:\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "#Define the chi-squared function (don't change this chunk of code), see above for explanation.\n",
        "def chi2(data, unc, model):\n",
        "  N = 1.00*len(data)\n",
        "  result = np.sum((data-model)**2/unc**2)/N\n",
        "  return result\n",
        "\n",
        "\n",
        "\n",
        "#input experimental data:\n",
        "\n",
        "x = np.array([1, 2, 3, 4, 5, 6]); #edit this line, these are made-up\n",
        "y = np.array([2.3, 4.1, 6.7, 8.3, 10.5, 13]) #edit this line, these are made-up\n",
        "unc = np.array([0.2, 0.3, 0.1, 0.2, 0.3, 0.2]) #edit this line, these are made-up\n",
        "\n",
        "# We will use linear fits in lab, so we set polynomial_degree = 1\n",
        "polynomial_degree = 1 ;\n",
        "\n",
        "#Do the fitting.\n",
        "p = np.polyfit(x, y, polynomial_degree, w = 1/unc) ;  # make the fit, no need to edit\n",
        "f = np.poly1d(p) # convert the fit coefficients into a function, no need to edit\n",
        "\n",
        "#Print some of the results:\n",
        "print('The fit slope is ', p[0]) # no need to edit these printout lines.\n",
        "print('The fit intercept is ', p[1])\n",
        "print('The chi-squared of this fit is ', chi2(y, unc, f(x)))\n",
        "\n",
        "#Plot:\n",
        "plt.rcParams.update({'font.size': 15})\n",
        "plt.errorbar(x, y, yerr = unc, xerr = None, fmt = 'k.'); #plot the data points\n",
        "plt.plot(x, f(x), 'g-') #plot the fit line\n",
        "plt.xlabel('x [units?]') #change this to correspond to what you measured\n",
        "plt.ylabel('y [units?]')  #change this to correspond to what you measured\n",
        "plt.legend(['Fit', 'Data']); #edit as you need\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
